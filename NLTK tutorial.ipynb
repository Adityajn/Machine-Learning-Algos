{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "1. NLTK is natural language toolkit.\n",
    "2. used for processing natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "print(nltk.__file__)\n",
    "\n",
    "#pip3 install nltk #to download nltk\n",
    "#nltk.download() #to download librarirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer\n",
    "1. word tokenizer - tokenize by word\n",
    "2. sentence tokenizer - tokenize by sentence\n",
    "\n",
    "## corpora\n",
    "it is body of text \n",
    "eg medical general, presidential speeches, english language\n",
    "\n",
    "## lexicon\n",
    "words and their meaning\n",
    "eg invester speak or english-speak\n",
    "1. for inverster 'bull' means someone who is +ve about market\n",
    "2. for english it is an animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = \"Hello Mr. Smith, how are you doing today? The weather is great and python is awesome. The sky is pinkish-blue. You should not get cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenizer\n",
    "\"\"\"\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stop words = filler words\n",
    "the, is, a, and, or etc \n",
    "\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#print(stop_words) #list of stopwords\n",
    "words = word_tokenize(example_text.lower())\n",
    "filtered_sentence = [w for w in words if not w in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stemming = remove similar words\n",
    "example write,writing,written\n",
    "\"\"\"\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"run\",\"running\",\"ran\",\"runs\",\"ride\",\"runly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "# this sometime gives bad result,\n",
    "# so there is another thing called wordnetLemmatizer\n",
    "# we will discuss later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tag list:\n",
    "\n",
    "1. CC\tcoordinating conjunction\n",
    "2. CD\tcardinal digit\n",
    "3. DT\tdeterminer\n",
    "4. EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "5. FW\tforeign word\n",
    "6. IN\tpreposition/subordinating conjunction\n",
    "7. JJ\tadjective\t'big'\n",
    "8. JJR\tadjective, comparative\t'bigger'\n",
    "9. JJS\tadjective, superlative\t'biggest'\n",
    "10. LS\tlist marker\t1)\n",
    "11. MD\tmodal\tcould, will\n",
    "12. NN\tnoun, singular 'desk'\n",
    "13. NNS\tnoun plural\t'desks'\n",
    "14. NNP\tproper noun, singular\t'Harrison'\n",
    "15. NNPS\tproper noun, plural\t'Americans'\n",
    "16. PDT\tpredeterminer\t'all the kids'\n",
    "17. POS\tpossessive ending\tparent's\n",
    "18. PRP\tpersonal pronoun\tI, he, she\n",
    "19. PRP$\\phi$\tpossessive pronoun\tmy, his, hers\n",
    "20. RB\tadverb\tvery, silently,\n",
    "21. RBR\tadverb, comparative\tbetter\n",
    "22. RBS\tadverb, superlative\tbest\n",
    "23. RP\tparticle\tgive up\n",
    "24. TO\tto\tgo 'to' the store.\n",
    "25. UH\tinterjection\terrrrrrrrm\n",
    "26. VB\tverb, base form\ttake\n",
    "27. VBD\tverb, past tense\ttook\n",
    "28. VBG\tverb, gerund/present participle\ttaking\n",
    "29. VBN\tverb, past participle\ttaken\n",
    "30. VBP\tverb, sing. present, non-3d\ttake\n",
    "31. VBZ\tverb, 3rd person sing. present\ttakes\n",
    "32. WDT\twh-determiner\twhich\n",
    "33. WP\twh-pronoun\twho, what\n",
    "34. WP$\\phi$\tpossessive wh-pronoun\twhose\n",
    "35. WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chunking\n",
    "find out meaning of sentence\n",
    "or grouping of words\n",
    "\"\"\"\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text =  state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            return\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            #print(chunked)\n",
    "            #chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "print(process_content())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chunking example for single sentence\n",
    "Play with it, easy\n",
    "\"\"\"\n",
    "sent = \"He is a great person, silently working George\"\n",
    "words = nltk.word_tokenize(sent)\n",
    "tagged = nltk.pos_tag(words)\n",
    "print(tagged)\n",
    "print()\n",
    "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<JJ>*<NN.?>}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "#joining chunked sentence\n",
    "print(chunked)\n",
    "print(\"\\nJoin chunked sentence\\n\")\n",
    "' '.join([w for w,t in chunked.leaves()])\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chinking\n",
    "opposite of chinkng\n",
    "\"\"\"\n",
    "examples = [\"he is silently working\",\"lets work with him\",\"loudly working George\"]\n",
    "for sent in examples:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    \n",
    "    chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                             }<VB.?|IN>{\"\"\"\n",
    "    chunkParser = nltk.RegexpParser(chunkGram)\n",
    "    chunked = chunkParser.parse(tagged)\n",
    "    print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Named entity recognition\n",
    "\n",
    "Types:\n",
    "ORGANIZATION    WHO, UN\n",
    "PERSON     President OBAMA\n",
    "LOCATION    Mount Everest, Murray River\n",
    "DATE  June, 2008-06-29\n",
    "TIME     1:30 pm, two fifty am\n",
    "PERCENT   twenty pct, 18.75%\n",
    "FACILITY    Washington Monument, StoneHedge\n",
    "GPE     South East Asia, Midlothian\n",
    "\"\"\"\n",
    "examples = [\"Aditya is silently working in White House\",\n",
    "            \"lets work with him\",\n",
    "            \"loudly working George in Boston\"]\n",
    "for sent in examples:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    \n",
    "    namedEnt = nltk.ne_chunk(tagged)\n",
    "    namedEnt.draw()\n",
    "    print(namedEnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lemmatizer  \n",
    "very similar to stemming\n",
    "but end result will be a real word, root word will be similar word with same meaning\n",
    "\n",
    "pos = type of word\n",
    "\n",
    "v = verb\n",
    "a = adjective\n",
    "n = noun\n",
    "\"\"\"\n",
    "example_words = [\"run\",\"running\",\"ran\",\"runs\",\"better\",\"best\",\"ride\",\"rode\",\"runly\"]\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in example_words:\n",
    "    print(lemmatizer.lemmatize(word,pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'): \n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ') or tag.startswith('RB'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        else:\n",
    "            yield word\n",
    "            \n",
    "sentence = \"hi, Aditya am a greater, better teacher and also i took it as a hardest working person\"\n",
    "print(nltk.pos_tag(word_tokenize(sentence)))\n",
    "print(' '.join(lemmatize_all(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "corpus\n",
    "\n",
    "corpus is located in nltk_data directory\n",
    "it contain different datasets\n",
    "\"\"\"\n",
    "from nltk.corpus import gutenberg\n",
    "sample = gutenberg.raw('bible-kjv.txt')\n",
    "print(sample[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wordnet = largest capability corpora\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#get synonyms set for word 'program'\n",
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns)\n",
    "\n",
    "#definition of word\n",
    "print(syns[0].definition())\n",
    "\n",
    "#examples\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(synonyms)\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"comparing word similarity\"\"\"\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "print(w2.lemmas()[0].name(),w1.lemmas()[0].name())\n",
    "print(w1.wup_similarity(w2))\n",
    "print()\n",
    "\n",
    "#lets try more\n",
    "w1 = wordnet.synset(\"good.n.01\")\n",
    "w2 = wordnet.synset(\"bad.n.01\")\n",
    "print(w2.lemmas()[0].name(),w1.lemmas()[0].name())\n",
    "print(w1.wup_similarity(w2))\n",
    "print()\n",
    "\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"car.n.01\")\n",
    "print(w2.lemmas()[0].name(),w1.lemmas()[0].name())\n",
    "print(w1.wup_similarity(w2))\n",
    "print()\n",
    "\n",
    "w1 = wordnet.synset(\"dog.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "print(w2.lemmas()[0].name(),w1.lemmas()[0].name())\n",
    "print(w1.wup_similarity(w2))\n",
    "print()\n",
    "\n",
    "w1 = wordnet.synset(\"aeroplane.n.01\")\n",
    "w2 = wordnet.synset(\"cactus.n.01\")\n",
    "print(w2.lemmas()[0].name(),w1.lemmas()[0].name())\n",
    "print(w1.wup_similarity(w2))\n",
    "print()\n",
    "\n",
    "#used to detect piracy, plagiarism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Text classification\n",
    "eg spam ham, sentiment analysis\n",
    "\"\"\"\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#documents = [( list(movie_reviews.words(fileid)),category)\n",
    "#            for category in movie_reviews.categories()\n",
    "#            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "#random.shuffle(documents)\n",
    "#documents[:5]\n",
    "short_pos = open(\"datasets/positive.txt\",\"r\",encoding='latin2').read()\n",
    "short_neg = open(\"datasets/negative.txt\",\"r\",encoding='latin2').read()\n",
    "documents =[]\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append((r,\"pos\"))\n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append((r,\"neg\"))\n",
    "all_words  = [] \n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features =  list(all_words.keys())[:5000]\n",
    "def find_features(document):\n",
    "    #words = set(document)\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = w in words\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(rev),category) for (rev,category) in documents]\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:10000]\n",
    "testing_set = featuresets[10000:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(classifier,testing_set)))\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "mnb = SklearnClassifier(MultinomialNB())\n",
    "mnb.train(training_set)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(mnb,testing_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gnb = SklearnClassifier(GaussianNB())\n",
    "#gnb.train(training_set)\n",
    "#print(\"Accuracy :\",(nltk.classify.accuracy(gnb,testing_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = SklearnClassifier(BernoulliNB())\n",
    "bnb.train(training_set)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(bnb,testing_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "lr = SklearnClassifier(LogisticRegression())\n",
    "lr.train(training_set)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(lr,testing_set)))\n",
    "\n",
    "sgd = SklearnClassifier(SGDClassifier())\n",
    "sgd.train(training_set)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(sgd,testing_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,NuSVC,LinearSVC\n",
    "svc = SklearnClassifier(SVC())\n",
    "svc.train(training_set)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(svc,testing_set)))\n",
    "\n",
    "nusvc = SklearnClassifier(NuSVC())\n",
    "nusvc.train(training_set)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(nusvc,testing_set)))\n",
    "\n",
    "lsvc = SklearnClassifier(LinearSVC())\n",
    "lsvc.train(training_set)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(lsvc,testing_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self,*classifiers):\n",
    "        self.classifiers = classifiers\n",
    "        \n",
    "    def classify(self,features):\n",
    "        votes = []\n",
    "        for c in self.classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self,features):\n",
    "        votes = []\n",
    "        for c in self.classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "voting_classifier = VoteClassifier(mnb,bnb,sgd,svc,nusvc,lsvc,lr)\n",
    "print(\"Accuracy :\",(nltk.classify.accuracy(voting_classifier,testing_set)))\n",
    "for i in [3,7,1,9,10,58,6]:\n",
    "    print(\"Classification: \",voting_classifier.classify(testing_set[i][0]),\n",
    "      \"Confidence: \",voting_classifier.confidence(testing_set[i][0])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge tweepy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401\n",
      "401\n",
      "401\n",
      "401\n",
      "401\n",
      "401\n"
     ]
    }
   ],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import json\n",
    "\n",
    "#consumer key, consumer secret, access token, access secret.\n",
    "ckey=\"asdfsafsafsaf\"\n",
    "csecret=\"asdfasdfsadfsa\"\n",
    "atoken=\"asdfsadfsafsaf-asdfsaf\"\n",
    "asecret=\"asdfsadfsadfsadfsadfsad\"\n",
    "\n",
    "class listener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        all_data = json.loads(data)\n",
    "        tweet = all_data[\"text\"]\n",
    "        #username = all_data[\"user\"][\"screen_name\"]\n",
    "        print((username,tweet))\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "auth = OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "\n",
    "twitterStream = Stream(auth, listener())\n",
    "twitterStream.filter(track=[\"car\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
