{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "1. What is Dimensionality Reduction?\n",
    "    1. Dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.\n",
    "    2. Feature Extraction - Feature selection approaches try to find a subset of the original variables (also called features or attributes). There are three strategies: \n",
    "        1. Filter strategy (e.g. information gain), \n",
    "        2. Wrapper strategy (e.g. search guided by accuracy)\n",
    "        3. Embedded strategy (features are selected to add or be removed while building the model based on the prediction errors).\n",
    "    3. Feature projection transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.\n",
    "\n",
    "2. Why Dimensionality Reduction?\n",
    "    1. Space Efficiency - not everyone has huge space to store all dimensions\n",
    "    2. Computing efficiency - smaller the data, faster are the model\n",
    "    3. Visualization - we cannot visualize more than 2-3 dimensions easily\n",
    "    4. Helps to remove Reduntant Features\n",
    "\n",
    "3. Important Dimensionality Reduction Methods\n",
    "    1. PCA - Principal Component Analysis\n",
    "    2. t-SNE - T-distributed Stochastic Neighbor Embedding \n",
    "    3. LDA - Linear Discriminant Analysis\n",
    "        \n",
    "4. More Resources\n",
    "    1. [PCA vs LDA](https://plot.ly/ipython-notebooks/principal-component-analysis/)\n",
    "    2. [Implementing PCA](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)\n",
    "    3. [PCA vs LDA](https://www.quora.com/What-is-the-difference-between-LDA-and-PCA-for-dimension-reduction)\n",
    "    4. [Adv of t-SNE over PCA](https://www.quora.com/What-advantages-does-the-t-SNE-algorithm-have-over-PCA)\n",
    "    5. [Intro to t-SNE](https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
